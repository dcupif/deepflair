{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFlair - A deep learning-based system to autoflair Reddit submissions\n",
    "\n",
    "Reddit's Automoderator has gone insane and has fully taken over r/Science. You are mandated by the all-powerful robot to build a new module allowing it to automatically (and correctly) flair all new submissions to the subreddit.\n",
    "\n",
    "On reddit, a flair is the tag given to a post. For instance, 'medicine' and 'biology' are possible flairs you will find on r/science. You can visit http://reddit.com/r/science for a better example, each post's flair being found to the left of the link to its comments.\n",
    "\n",
    "The specifications are as follow:\n",
    "* All code must be in python\n",
    "* Extract at least 2,000 recent r/Science posts and use them to fit your classifier.\n",
    "\n",
    "***WARNING: data collection will be a bit tough because Reddit’s official API no longer allows to extract posts from subs. You’ll have to be creative. Please send us your thoughts by email as soon as you have a solution so we can guide you if necessary. Don’t spend more than 20 minutes on finding a way to collect the data.***\n",
    "\n",
    "\n",
    "As a second test, run your classifier on a set of at least 500 posts from the month of January to prove its real-life usefulness.\n",
    "* Write a short (~2 pages) report detailing how you gathered data, your choice of tools, classification algorithm, hyper-parameter and your evaluation techniques. Present and discuss your results and give a possible explanations for them. Also suggest possible improvements.\n",
    "* Quickly answer the following questions:\n",
    "    * How would you make your classifier available as a web service?\n",
    "    * Would it be possible to improve the results using the content of a submission's comments? (bear in mind r/Science is highly moderated and non-topical discussions are removed) If so, how?\n",
    "* Deliver the following:\n",
    "    * The data used\n",
    "    * All the code used for the task\n",
    "    * ~2 pages report + answers to the questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "I'm going to explain here what I understood for the problem, and give a clear definition of the problem statement that I will try to answer.\n",
    "\n",
    "### Problem Definition\n",
    "\n",
    "Reddit is an American social news aggregation, web content rating, and discussion website. Users can submit content to the site such as links, text posts, and images, which are then voted up or down by other members. This content can be organized in \"subreddits\" which can be seen as channels allowing discussions on a specific topic. In this assignment, we will focus on the r/science subreddit.\n",
    "\n",
    "Each submission on Reddit can be tagged, i.e. flaired, with a keyword. 'medicine' and 'biology' are examples given of possible flairs that can be found under the r/science subreddit.\n",
    "\n",
    "The problem is the following:\n",
    "\n",
    "**Given the content and metadata of a submission under the r/science subreddit, design an automated system capable of tagging the specific post with a single relevant keyword.**\n",
    "\n",
    "### Example\n",
    "\n",
    "Given the following post (taken from today's r/science posts):\n",
    "\n",
    "*\"Scientists have devised a \"double Trojan horse\" drug that fools antibiotic-resistant bacteria into committing suicide. The drug appears to be a nutrient, but it contains two antibiotics. When the bacterium destroys the first antibiotic, it unleashes the second antibiotic, killing it.\"*\n",
    "\n",
    "Our system should be able to assign automatically the tag **'medicine'**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and Solving Strategy\n",
    "\n",
    "### Observations\n",
    "\n",
    "Natural Language Processing (NLP) is a science designed specifically to tackle such tasks. It will allow us to extract the essence of each post, and automatically understand the topic.\n",
    "\n",
    "As a subfield of deep learning, a NLP problem usually requires a lot of training data to achieve good performance. Thus, we will need to collect a good amount of Reddit submissions in order to train our model. Here, the assignment specifies we will need to collect at least **~2,000 posts** to fit our classifier.\n",
    "\n",
    "We will need a finite number of classes for our classifier. This means we need to identify exhaustively the different tags/flairs that can be attributed to a post.\n",
    "\n",
    "Our data will need to be clearly labeled with ground-truth tags. It seems that **each post can be assigned only one tag**, i.e. one class.\n",
    "\n",
    "In this assignment, we don't really need to understand the meaning of each post, we just need to assign them a topic label. For this reason, my first thought is to try a very dense simple deep neural net without even considering concepts such as 'context' with more powerful recurrent neural nets like GRU or LSTM.\n",
    "\n",
    "However, we would like our system to be able to react positively to words it has not seen before. For example, if our training set of examples contains posts with the word 'cancer' and are tagged with the word 'medicine', we would like our system to be able to recognize an unseen post containing the word 'tumor' as a 'medicine' post as well. We will use **word embeddings** to answer this. \n",
    "\n",
    "### Solving Strategy\n",
    "\n",
    "It is always better to start with simple models first and then iterate to improve performance. This gives us a good start to work with and improve. Here are the following steps we will implement:\n",
    "\n",
    "1. Data collection   \n",
    "2. Create a simple model\n",
    "3. Train it\n",
    "4. Evaluate the model\n",
    "\n",
    "Let's start!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "The assignement lets us know that the Reddit API does not allow to extract posts from subreddits anymore, so we will use the API from pushshift.io instead. (https://github.com/pushshift/api)\n",
    "\n",
    "We will start by just tickling around and see what kind of information we can get. This will help us decide which kind of metadata we would like to use to train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"author\": \"jalovisko\",\n",
      "      \"author_flair_css_class\": null,\n",
      "      \"author_flair_richtext\": [],\n",
      "      \"author_flair_text\": null,\n",
      "      \"author_flair_type\": \"text\",\n",
      "      \"can_mod_post\": false,\n",
      "      \"contest_mode\": false,\n",
      "      \"created_utc\": 1526159182,\n",
      "      \"domain\": \"skoltech.ru\",\n",
      "      \"full_link\": \"https://www.reddit.com/r/science/comments/8iyxub/neural_network_trained_to_assess_fire_effects/\",\n",
      "      \"id\": \"8iyxub\",\n",
      "      \"is_crosspostable\": true,\n",
      "      \"is_original_content\": false,\n",
      "      \"is_reddit_media_domain\": false,\n",
      "      \"is_self\": false,\n",
      "      \"is_video\": false,\n",
      "      \"link_flair_background_color\": \"#d982cb\",\n",
      "      \"link_flair_css_class\": \"compsci\",\n",
      "      \"link_flair_richtext\": [\n",
      "        {\n",
      "          \"e\": \"text\",\n",
      "          \"t\": \"Computer Science\"\n",
      "        }\n",
      "      ],\n",
      "      \"link_flair_template_id\": \"6462d546-889b-11e3-9380-12313b0ce8a6\",\n",
      "      \"link_flair_text\": \"Computer Science\",\n",
      "      \"link_flair_text_color\": \"light\",\n",
      "      \"link_flair_type\": \"richtext\",\n",
      "      \"locked\": false,\n",
      "      \"no_follow\": true,\n",
      "      \"num_comments\": 0,\n",
      "      \"num_crossposts\": 0,\n",
      "      \"over_18\": false,\n",
      "      \"parent_whitelist_status\": \"all_ads\",\n",
      "      \"permalink\": \"/r/science/comments/8iyxub/neural_network_trained_to_assess_fire_effects/\",\n",
      "      \"pinned\": false,\n",
      "      \"pwls\": 6,\n",
      "      \"retrieved_on\": 1526159184,\n",
      "      \"rte_mode\": \"markdown\",\n",
      "      \"score\": 1,\n",
      "      \"selftext\": \"\",\n",
      "      \"send_replies\": true,\n",
      "      \"spoiler\": false,\n",
      "      \"stickied\": false,\n",
      "      \"subreddit\": \"science\",\n",
      "      \"subreddit_id\": \"t5_mouw\",\n",
      "      \"subreddit_subscribers\": 18646117,\n",
      "      \"subreddit_type\": \"public\",\n",
      "      \"suggested_sort\": \"confidence\",\n",
      "      \"thumbnail\": \"default\",\n",
      "      \"title\": \"Neural network trained to assess fire effects.\",\n",
      "      \"url\": \"https://www.skoltech.ru/en/2018/04/neural-network-trained-to-assess-fire-effects/\",\n",
      "      \"whitelist_status\": \"all_ads\",\n",
      "      \"wls\": 6\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "url_base = \"https://api.pushshift.io/\"\n",
    "\n",
    "def get_posts(subreddit, before=0, size=500):\n",
    "    request_url = \"{}/reddit/search/submission/?subreddit={}&before={}d&size={}\".format(url_base, subreddit, before, size)\n",
    "    response = requests.get(request_url)\n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.content.decode('utf-8'))\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "last_post = get_posts(\"science\", size=1)\n",
    "\n",
    "if last_post:\n",
    "    print(json.dumps(last_post, sort_keys=True, indent=2))\n",
    "else:\n",
    "    print('[!] Request Failed')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of information here!\n",
    "\n",
    "After some investigation, we realize that the flair information is contained in the *link_flair_text* key. Bad news, this field is not always present so we will have to take care of this case. Good news? We found our labels. This is what we will try to predict with our model.\n",
    "\n",
    "Let's see now which information could help us train a model to perform this task.\n",
    "\n",
    "Apparently, the only interesting information we can get from this is the title (*title* key). This seems the most promising as it is kind of a summary for each submission. It contains text with relevant words that can be linked to the post topic.\n",
    "\n",
    "The URL might also contain some keywords that could help classify a post, but it seems less promising: the url is not necessary connected to the post content, and it will contain a lot of noise anyway.\n",
    "\n",
    "We will keep focused on the information contained in the title to train our model.\n",
    "\n",
    "### Collecting a lot of submissions\n",
    "\n",
    "We observe that the pushshift API does not let us return more than 500 hundred results per request. Remember, we want to collect at least 2,000 posts. Let's aim for 10,000 to have a decent number of training examples.\n",
    "\n",
    "We will collect those examples from different time periods. That way, we'll be sure all our examples are different, and we would not want our classifier to overfit a specific period of time.\n",
    "\n",
    "Let's extract 500 posts from each month over the last three years: 500*36 = 18,000 posts. I initially tried with two years of data, but many posts are not labeled so I just took more data to get close to the 10,000 examples we aim for.\n",
    "\n",
    "We will preprocess the data to keep only the title and flair. The collected data will be dumped into a csv file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 9742\n",
      "Number of classes: 128\n",
      "Classes: {'Flu AMA', 'Planetary Science AMA', 'Monsanto AMA', 'Autism AMA', 'Honey Bee Genome AMA', 'Chemistry AMA', 'Stress and the Brain AMA', 'Science Discussion', 'Science Writing AMA', 'Weatherman AMA', 'Social Sciences', 'Open Access AMA', 'NOAA AMA', 'Hurricane Prediction AMA', 'Clean Water AMA', 'Computer Science', 'Suicide Prevention AMA', \"People's Climate March AMA\", 'Computer Sci', 'Cholera AMA', 'DNA Day Series | National Society of Genetic Counselors', 'Health AMA', 'Virtual Reality AMA', 'Open Science AMA', 'Forensic Chemistry AMA', 'Plasma Physics AMA', 'Climate Change AMA', 'Fetal Tissue Research AMA', 'Self-treating ALS AMA', 'Zealandia expedition AMA', 'Chronic Pain AMA', 'Unlock Your Genome AMA', 'Subreddit News', 'In Mice', 'Economics', 'Psychology', 'Human Genome AMA', 'Snow and Ice AMA', 'Physics', 'Climate Science AMA', 'Misleading Title', 'Biosimilars AMA', 'GMO AMA', 'DNA Day Series | The Cancer Genome Atlas', 'Anthropology', 'Environment', 'Social Science', 'Landfill AMA', 'Astronomy', 'Biofuel AMA', 'Paleoanthropology AMA', 'Vector-borne diseases AMA', 'Solar Astronomers AMA', 'Science Outreach AMA', 'Web of Science AMA', 'Sexually transmitted infections (STIs) AMA', 'Paleontology', 'Exercise AMA', 'Thanksgiving Chemistry AMA', 'Chemistry Journalism AMA', 'Microbial Control AMA', 'Neuroscience', 'ACS AMA', 'Public Health AMA', 'Open Access Journal AMA', 'Neuroscience AMA', 'Honey Bee AMA', 'Black Hole AMA', 'Nutrition AMA', 'Facial Recognition AMA', 'Election Science AMA', 'Animal Cognition AMA', 'Ecology AMA', 'Geology', 'Paleontology AMA', 'Health', 'Medicine', 'Permafrost AMA', 'Mental Fatigue AMA', 'Nanoscience', 'Cannabis Chemistry AMA', 'TianjinExplosion AMA', 'Copyright AMA', 'Nanoparticle AMA', 'Sugar AMA', 'Cyborg Insect AMA', 'Mental Health Services AMA', 'Human Cell Atlas  AMA', 'Forest restoration AMA', 'Ebola AMA', 'Climate Science Report AMA', 'Toxicology Biomarkers AMA', 'Porn AMA', 'Engineering', 'Solar Eclipse AMA', 'Spaceship AMA', 'Cremation Technology AMA', 'Special Message', 'Biology', 'Alzheimer’s disease AMA', 'Epidemiology', 'Hurricane Patricia AMA', 'Peer Review AMA', 'Mathematics', 'Computational Science | US National Labs AMA', 'Fire &amp; Rescue AMA', 'Carbon-Capture AMA', 'Cancer AMA', 'Computational Chemistry/Biology AMA', 'Medical AMA', 'Astronomy AMA', 'Science Friday AMA', 'Psychopathic Behavior', 'Neurobiology Of Addictive Disorders AMA', 'Biotechnology AMA', 'False Memory AMA', 'PLOS AMA', 'Zika AMA', 'Animal Science', 'Poor Title', 'Chemistry', 'Science Breakthrough AMA', 'Artificial Intelligence AMA', 'Earth Science', 'Neuroscience of Sleep AMA', 'Cancer', 'Net Neutrality', 'Biology AMA'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Key names we want to keep\n",
    "fieldnames = [\"title\", \"link_flair_text\"]\n",
    "\n",
    "def format_posts(posts):\n",
    "    res = []\n",
    "    for post in posts:\n",
    "        res.append({key: post[key] for key in fieldnames if key in post})\n",
    "    return res\n",
    "\n",
    "\n",
    "def remove_posts_without_label(posts):\n",
    "    return [post for post in posts if fieldnames[1] in post]\n",
    "\n",
    "\n",
    "def get_data(months):\n",
    "    data = []\n",
    "    for i in range(months):\n",
    "        posts = get_posts(\"science\", before=i*30, size=500)\n",
    "        posts = format_posts(posts[\"data\"])\n",
    "        posts = remove_posts_without_label(posts)\n",
    "        data.extend(posts)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_classes(posts):\n",
    "    return set([post[fieldnames[1]] for post in posts])\n",
    "\n",
    "\n",
    "def change_flair_to_label(posts, flair_to_label):\n",
    "    for post in posts:\n",
    "        post[fieldnames[1]] = flair_to_label[post[fieldnames[1]]]\n",
    "\n",
    "\n",
    "# Get, format and clean the data\n",
    "posts = get_data(36)\n",
    "\n",
    "# Determine the classes\n",
    "classes = get_classes(posts)\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Define two dictionaries mapping label to flair and flair to label\n",
    "flair_to_label = {c:i for (i,c) in enumerate(classes)}\n",
    "label_to_flair = {i:c for (i,c) in enumerate(classes)}\n",
    "\n",
    "# Now we have our mapping, we can change the flair to its associated label\n",
    "change_flair_to_label(posts, flair_to_label)\n",
    "\n",
    "# Some nice information about our dataset\n",
    "print(\"Number of training examples: {}\".format(len(posts)))\n",
    "print(\"Number of classes: {}\".format(num_classes))\n",
    "print(\"Classes: {}\". format(classes))\n",
    "\n",
    "# We save the data in a csv file\n",
    "with open(\"data.csv\", \"w\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(posts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At end of this step, we manage to collect around ~10,000 examples from the past three years, correctly labeled. There are ~130 different classes / different tags. We now have a good dataset to work with and train a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "\n",
    "As we said earlier, we probably don't need to use RNNs like LSTM or GRU networks right away. It seems to me that the importance of the context here is not that important since we want to assign a general topic to each post.\n",
    "\n",
    "If we had to care about things like negations in the content, to perform sentiment analysis for example, we would probably have to care about context. (ex: \"I am not happy\" --> our system would fail to recognize the negation whereas LSTMs would be able to take this into account. But here it seems fine because we won't have so many cases likes this)\n",
    "\n",
    "On the other hand, we would like our system to classify correctly posts that use vocabulary unseen in the training set. If the model trained on specific medicine terminology, using word embeddings would allow us to associate new medicine terminology with the one our system trained on.\n",
    "\n",
    "Below is an image showcasing our first design idea for our model:\n",
    "\n",
    "![Model Design](./20180512_133744.jpg)\n",
    "\n",
    "We'll take the posts' titles as input and transform them into their vector representations. For this, we will use pre-trained word embeddings to avoid training overhead. This will be a good enough start.\n",
    "\n",
    "We'll then compute the average on these words embeddings, and feed the resulting vector to a dense neural net. The output of this network will be a vector of probabilities for each class of shape (num_classes, 1). We will have to convert our labels to their corresponding one-hot representations to match the network output.\n",
    "\n",
    "Finally, we'll predict the tag of unseen posts by taking the maximum value of the vector of probabilities.\n",
    "\n",
    "# Data pre-processing\n",
    "\n",
    "There is still some processing to be done to prepare our data. Let's load everything in a pandas dataframe first and take a look at what we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  link_flair_text\n",
      "0     Neural network trained to assess fire effects.               15\n",
      "1  So you can never out run your cat and your dog...              118\n",
      "2  Effects of N-acetylcysteine on marijuana depen...               76\n",
      "3      Omeprazole increases the risk of heart attack               76\n",
      "4  Stephen Hawking service: Possibility of time t...               48\n",
      "5  Teachers who antagonize their students by beli...               35\n",
      "6  It's tobacco and alcohol use - not illegal dru...               75\n",
      "7  Fitness apps found to make almost no differenc...               75\n",
      "8  Google DeepMind's AI learns navigation skills ...               15\n",
      "9  People using brain-computer interface are more...               61\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data.csv\", quotechar='\"', skipinitialspace=True)\n",
    "\n",
    "print(df[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text pre-processing\n",
    "\n",
    "We need to prepare and clean the title text data thanks to Keras pre-processing tools. We are going to map words to indexes and pad the different sequences to the maximum length title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19441 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# We extract the text data from the title column of our dataframe\n",
    "texts = df[fieldnames[0]].as_matrix()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens.'.format(len(word_index)))\n",
    "\n",
    "maxlen = len(max(texts, key=len).split())\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels pre-processing\n",
    "\n",
    "The text is now properly cleaned and ready to be fed to the neural network. Let's now transform our labels to their one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "labels = df[fieldnames[1]]\n",
    "labels = to_categorical(labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the shape of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (9742, 44)\n",
      "Shape of label tensor: (9742, 128)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor: {}'.format(data.shape))\n",
    "print('Shape of label tensor: {}'.format(labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split our data into a training set and a validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(0.2 * data.shape[0])\n",
    "\n",
    "X_train = data[:-nb_validation_samples]\n",
    "Y_train = labels[:-nb_validation_samples]\n",
    "X_val = data[-nb_validation_samples:]\n",
    "Y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation\n",
    "\n",
    "It is now time to implement our model. We will first prepare the embedding layer by loading pre-trained 50-dimensional Glove word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Dense, Dropout, Embedding, Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Compute an index mapping words to known embeddings, by parsing the data dump of pre-trained\n",
    "# 50-dimensional GloVe embeddings.\n",
    "embeddings_index = {}\n",
    "with open('data/glove.6B.50d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found {} word vectors.'.format(len(embeddings_index)))\n",
    "\n",
    "# Compute the embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 50))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# Prepare the embedding_layer\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            50,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally ready to define the model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7794 samples, validate on 1948 samples\n",
      "Epoch 1/100\n",
      "7794/7794 [==============================] - 1s 99us/step - loss: 2.9042 - acc: 0.1946 - val_loss: 2.1754 - val_acc: 0.3157\n",
      "Epoch 2/100\n",
      "7794/7794 [==============================] - 0s 50us/step - loss: 2.2416 - acc: 0.3140 - val_loss: 2.0223 - val_acc: 0.3650\n",
      "Epoch 3/100\n",
      "7794/7794 [==============================] - 0s 51us/step - loss: 2.1308 - acc: 0.3396 - val_loss: 1.9657 - val_acc: 0.3937\n",
      "Epoch 4/100\n",
      "7794/7794 [==============================] - 0s 49us/step - loss: 2.0804 - acc: 0.3546 - val_loss: 1.9374 - val_acc: 0.3989\n",
      "Epoch 5/100\n",
      "7794/7794 [==============================] - 1s 68us/step - loss: 2.0435 - acc: 0.3655 - val_loss: 1.9100 - val_acc: 0.4158\n",
      "Epoch 6/100\n",
      "7794/7794 [==============================] - 1s 66us/step - loss: 2.0117 - acc: 0.3669 - val_loss: 1.9171 - val_acc: 0.3901\n",
      "Epoch 7/100\n",
      "7794/7794 [==============================] - 0s 52us/step - loss: 1.9932 - acc: 0.3673 - val_loss: 1.8972 - val_acc: 0.4174\n",
      "Epoch 8/100\n",
      "7794/7794 [==============================] - 0s 52us/step - loss: 1.9778 - acc: 0.3791 - val_loss: 1.9098 - val_acc: 0.3989\n",
      "Epoch 9/100\n",
      "7794/7794 [==============================] - 0s 48us/step - loss: 1.9468 - acc: 0.3879 - val_loss: 1.9067 - val_acc: 0.4040\n",
      "Epoch 10/100\n",
      "7794/7794 [==============================] - 1s 71us/step - loss: 1.9525 - acc: 0.3890 - val_loss: 1.8925 - val_acc: 0.4055\n",
      "Epoch 11/100\n",
      "7794/7794 [==============================] - 0s 47us/step - loss: 1.9266 - acc: 0.3904 - val_loss: 1.8856 - val_acc: 0.3999\n",
      "Epoch 12/100\n",
      "7794/7794 [==============================] - 0s 52us/step - loss: 1.9058 - acc: 0.3967 - val_loss: 1.8918 - val_acc: 0.4102\n",
      "Epoch 13/100\n",
      "7794/7794 [==============================] - 0s 51us/step - loss: 1.9008 - acc: 0.3953 - val_loss: 1.8906 - val_acc: 0.4189\n",
      "Epoch 14/100\n",
      "7794/7794 [==============================] - 0s 50us/step - loss: 1.8905 - acc: 0.3950 - val_loss: 1.8974 - val_acc: 0.4035\n",
      "Epoch 15/100\n",
      "7794/7794 [==============================] - 1s 74us/step - loss: 1.8703 - acc: 0.4066 - val_loss: 1.8991 - val_acc: 0.4127\n",
      "Epoch 16/100\n",
      "7794/7794 [==============================] - 0s 50us/step - loss: 1.8719 - acc: 0.4002 - val_loss: 1.8862 - val_acc: 0.4158\n",
      "Epoch 17/100\n",
      "7794/7794 [==============================] - 0s 49us/step - loss: 1.8674 - acc: 0.4103 - val_loss: 1.8830 - val_acc: 0.4071\n",
      "Epoch 18/100\n",
      "7794/7794 [==============================] - 0s 52us/step - loss: 1.8465 - acc: 0.4083 - val_loss: 1.8850 - val_acc: 0.4158\n",
      "Epoch 19/100\n",
      "7794/7794 [==============================] - 0s 57us/step - loss: 1.8480 - acc: 0.4048 - val_loss: 1.8762 - val_acc: 0.4158\n",
      "Epoch 20/100\n",
      "7794/7794 [==============================] - 1s 68us/step - loss: 1.8357 - acc: 0.4176 - val_loss: 1.8778 - val_acc: 0.4209\n",
      "Epoch 21/100\n",
      "7794/7794 [==============================] - 0s 51us/step - loss: 1.8162 - acc: 0.4244 - val_loss: 1.8782 - val_acc: 0.4230\n",
      "Epoch 22/100\n",
      "7794/7794 [==============================] - 0s 51us/step - loss: 1.8128 - acc: 0.4142 - val_loss: 1.8866 - val_acc: 0.4174\n",
      "Epoch 23/100\n",
      "7794/7794 [==============================] - 0s 50us/step - loss: 1.8111 - acc: 0.4188 - val_loss: 1.8893 - val_acc: 0.4168\n",
      "Epoch 24/100\n",
      "7794/7794 [==============================] - 0s 57us/step - loss: 1.8190 - acc: 0.4149 - val_loss: 1.8818 - val_acc: 0.4220\n",
      "Epoch 25/100\n",
      "7794/7794 [==============================] - 0s 61us/step - loss: 1.8112 - acc: 0.4219 - val_loss: 1.8947 - val_acc: 0.4117\n",
      "Epoch 26/100\n",
      "7794/7794 [==============================] - 0s 52us/step - loss: 1.8109 - acc: 0.4119 - val_loss: 1.8976 - val_acc: 0.4132\n",
      "Epoch 27/100\n",
      "7794/7794 [==============================] - 0s 51us/step - loss: 1.8100 - acc: 0.4134 - val_loss: 1.8904 - val_acc: 0.4189\n",
      "Epoch 28/100\n",
      "7794/7794 [==============================] - 0s 52us/step - loss: 1.8029 - acc: 0.4199 - val_loss: 1.8917 - val_acc: 0.4158\n",
      "Epoch 29/100\n",
      "7794/7794 [==============================] - 1s 72us/step - loss: 1.7933 - acc: 0.4234 - val_loss: 1.8803 - val_acc: 0.4148\n",
      "Epoch 30/100\n",
      "7794/7794 [==============================] - 0s 53us/step - loss: 1.7834 - acc: 0.4206 - val_loss: 1.8862 - val_acc: 0.4199\n",
      "Epoch 31/100\n",
      "7794/7794 [==============================] - 0s 51us/step - loss: 1.7952 - acc: 0.4189 - val_loss: 1.8872 - val_acc: 0.4097\n",
      "Epoch 32/100\n",
      "7794/7794 [==============================] - 0s 50us/step - loss: 1.7787 - acc: 0.4274 - val_loss: 1.8851 - val_acc: 0.4189\n",
      "Epoch 33/100\n",
      "7794/7794 [==============================] - 0s 52us/step - loss: 1.7913 - acc: 0.4275 - val_loss: 1.8792 - val_acc: 0.4143\n",
      "Epoch 34/100\n",
      "7794/7794 [==============================] - 1s 70us/step - loss: 1.7807 - acc: 0.4162 - val_loss: 1.8833 - val_acc: 0.4215\n",
      "Epoch 35/100\n",
      "7794/7794 [==============================] - 0s 52us/step - loss: 1.7788 - acc: 0.4231 - val_loss: 1.8877 - val_acc: 0.4153\n",
      "Epoch 36/100\n",
      "7794/7794 [==============================] - 0s 55us/step - loss: 1.7769 - acc: 0.4311 - val_loss: 1.8872 - val_acc: 0.4261\n",
      "Epoch 37/100\n",
      "7794/7794 [==============================] - 0s 56us/step - loss: 1.7664 - acc: 0.4246 - val_loss: 1.8850 - val_acc: 0.4168\n",
      "Epoch 38/100\n",
      "7794/7794 [==============================] - 0s 54us/step - loss: 1.7577 - acc: 0.4261 - val_loss: 1.8873 - val_acc: 0.4266\n",
      "Epoch 39/100\n",
      "7794/7794 [==============================] - 1s 69us/step - loss: 1.7525 - acc: 0.4298 - val_loss: 1.8856 - val_acc: 0.4168\n",
      "Epoch 40/100\n",
      "7794/7794 [==============================] - 1s 70us/step - loss: 1.7586 - acc: 0.4306 - val_loss: 1.8962 - val_acc: 0.4168\n",
      "Epoch 41/100\n",
      "7794/7794 [==============================] - 1s 66us/step - loss: 1.7546 - acc: 0.4251 - val_loss: 1.8884 - val_acc: 0.4230\n",
      "Epoch 42/100\n",
      "7794/7794 [==============================] - 1s 75us/step - loss: 1.7472 - acc: 0.4344 - val_loss: 1.8917 - val_acc: 0.4215\n",
      "Epoch 43/100\n",
      "7794/7794 [==============================] - 1s 66us/step - loss: 1.7502 - acc: 0.4332 - val_loss: 1.8985 - val_acc: 0.4168\n",
      "Epoch 44/100\n",
      "7794/7794 [==============================] - 0s 49us/step - loss: 1.7552 - acc: 0.4262 - val_loss: 1.8964 - val_acc: 0.4230\n",
      "Epoch 45/100\n",
      "7794/7794 [==============================] - 0s 64us/step - loss: 1.7447 - acc: 0.4311 - val_loss: 1.8955 - val_acc: 0.4235\n",
      "Epoch 46/100\n",
      "7794/7794 [==============================] - 1s 66us/step - loss: 1.7458 - acc: 0.4310 - val_loss: 1.8999 - val_acc: 0.4256\n",
      "Epoch 47/100\n",
      "7794/7794 [==============================] - 0s 52us/step - loss: 1.7462 - acc: 0.4357 - val_loss: 1.8993 - val_acc: 0.4209\n",
      "Epoch 48/100\n",
      "7794/7794 [==============================] - 0s 63us/step - loss: 1.7342 - acc: 0.4311 - val_loss: 1.9006 - val_acc: 0.4286\n",
      "Epoch 49/100\n",
      "7794/7794 [==============================] - 0s 61us/step - loss: 1.7425 - acc: 0.4311 - val_loss: 1.8982 - val_acc: 0.4179\n",
      "Epoch 50/100\n",
      "7794/7794 [==============================] - 0s 59us/step - loss: 1.7385 - acc: 0.4376 - val_loss: 1.8976 - val_acc: 0.4153\n",
      "Epoch 51/100\n",
      "7794/7794 [==============================] - 0s 52us/step - loss: 1.7216 - acc: 0.4394 - val_loss: 1.8997 - val_acc: 0.4215\n",
      "Epoch 52/100\n",
      "7794/7794 [==============================] - 0s 52us/step - loss: 1.7273 - acc: 0.4306 - val_loss: 1.9074 - val_acc: 0.4251\n",
      "Epoch 53/100\n",
      "7794/7794 [==============================] - 0s 50us/step - loss: 1.7267 - acc: 0.4398 - val_loss: 1.9037 - val_acc: 0.4266\n",
      "Epoch 54/100\n",
      "7794/7794 [==============================] - 0s 56us/step - loss: 1.7069 - acc: 0.4432 - val_loss: 1.9082 - val_acc: 0.4261\n",
      "Epoch 55/100\n",
      "7794/7794 [==============================] - 0s 55us/step - loss: 1.7161 - acc: 0.4415 - val_loss: 1.9095 - val_acc: 0.4220\n",
      "Epoch 56/100\n",
      "7794/7794 [==============================] - 0s 52us/step - loss: 1.7234 - acc: 0.4347 - val_loss: 1.9063 - val_acc: 0.4215\n",
      "Epoch 57/100\n",
      "7794/7794 [==============================] - 0s 48us/step - loss: 1.7149 - acc: 0.4374 - val_loss: 1.9074 - val_acc: 0.4204\n",
      "Epoch 58/100\n",
      "7794/7794 [==============================] - 0s 51us/step - loss: 1.7199 - acc: 0.4348 - val_loss: 1.9125 - val_acc: 0.4251\n",
      "Epoch 59/100\n",
      "7794/7794 [==============================] - 0s 54us/step - loss: 1.7161 - acc: 0.4397 - val_loss: 1.9065 - val_acc: 0.4225\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7794/7794 [==============================] - 0s 49us/step - loss: 1.7122 - acc: 0.4374 - val_loss: 1.9181 - val_acc: 0.4245\n",
      "Epoch 61/100\n",
      "7794/7794 [==============================] - 0s 44us/step - loss: 1.7110 - acc: 0.4415 - val_loss: 1.9124 - val_acc: 0.4174\n",
      "Epoch 62/100\n",
      "7794/7794 [==============================] - 0s 48us/step - loss: 1.7209 - acc: 0.4419 - val_loss: 1.9152 - val_acc: 0.4189\n",
      "Epoch 63/100\n",
      "7794/7794 [==============================] - 0s 44us/step - loss: 1.7164 - acc: 0.4376 - val_loss: 1.9164 - val_acc: 0.4276\n",
      "Epoch 64/100\n",
      "7794/7794 [==============================] - 0s 45us/step - loss: 1.7039 - acc: 0.4455 - val_loss: 1.9091 - val_acc: 0.4179\n",
      "Epoch 65/100\n",
      "7794/7794 [==============================] - 0s 44us/step - loss: 1.7043 - acc: 0.4396 - val_loss: 1.9192 - val_acc: 0.4225\n",
      "Epoch 66/100\n",
      "7794/7794 [==============================] - 0s 45us/step - loss: 1.7044 - acc: 0.4419 - val_loss: 1.9187 - val_acc: 0.4158\n",
      "Epoch 67/100\n",
      "7794/7794 [==============================] - 0s 45us/step - loss: 1.7042 - acc: 0.4342 - val_loss: 1.9200 - val_acc: 0.4261\n",
      "Epoch 68/100\n",
      "7794/7794 [==============================] - 0s 45us/step - loss: 1.6992 - acc: 0.4444 - val_loss: 1.9149 - val_acc: 0.4251\n",
      "Epoch 69/100\n",
      "7794/7794 [==============================] - 0s 43us/step - loss: 1.7032 - acc: 0.4405 - val_loss: 1.9175 - val_acc: 0.4215\n",
      "Epoch 70/100\n",
      "7794/7794 [==============================] - 0s 44us/step - loss: 1.6931 - acc: 0.4398 - val_loss: 1.9184 - val_acc: 0.4168\n",
      "Epoch 71/100\n",
      "7794/7794 [==============================] - 0s 47us/step - loss: 1.7130 - acc: 0.4353 - val_loss: 1.9209 - val_acc: 0.4266\n",
      "Epoch 72/100\n",
      "7794/7794 [==============================] - 0s 44us/step - loss: 1.6989 - acc: 0.4525 - val_loss: 1.9168 - val_acc: 0.4215\n",
      "Epoch 73/100\n",
      "7794/7794 [==============================] - 0s 44us/step - loss: 1.7085 - acc: 0.4385 - val_loss: 1.9228 - val_acc: 0.4240\n",
      "Epoch 74/100\n",
      "7794/7794 [==============================] - 0s 47us/step - loss: 1.6983 - acc: 0.4451 - val_loss: 1.9248 - val_acc: 0.4184\n",
      "Epoch 75/100\n",
      "7794/7794 [==============================] - 0s 45us/step - loss: 1.6990 - acc: 0.4468 - val_loss: 1.9205 - val_acc: 0.4215\n",
      "Epoch 76/100\n",
      "7794/7794 [==============================] - 0s 47us/step - loss: 1.6953 - acc: 0.4433 - val_loss: 1.9164 - val_acc: 0.4194\n",
      "Epoch 77/100\n",
      "7794/7794 [==============================] - 0s 47us/step - loss: 1.7028 - acc: 0.4451 - val_loss: 1.9217 - val_acc: 0.4266\n",
      "Epoch 78/100\n",
      "7794/7794 [==============================] - 0s 46us/step - loss: 1.6980 - acc: 0.4418 - val_loss: 1.9180 - val_acc: 0.4225\n",
      "Epoch 79/100\n",
      "7794/7794 [==============================] - 0s 47us/step - loss: 1.6959 - acc: 0.4442 - val_loss: 1.9240 - val_acc: 0.4199\n",
      "Epoch 80/100\n",
      "7794/7794 [==============================] - 0s 48us/step - loss: 1.6927 - acc: 0.4486 - val_loss: 1.9266 - val_acc: 0.4189\n",
      "Epoch 81/100\n",
      "7794/7794 [==============================] - 1s 71us/step - loss: 1.6894 - acc: 0.4459 - val_loss: 1.9274 - val_acc: 0.4245\n",
      "Epoch 82/100\n",
      "7794/7794 [==============================] - 1s 66us/step - loss: 1.6891 - acc: 0.4403 - val_loss: 1.9284 - val_acc: 0.4215\n",
      "Epoch 83/100\n",
      "7794/7794 [==============================] - 1s 70us/step - loss: 1.6830 - acc: 0.4455 - val_loss: 1.9277 - val_acc: 0.4281\n",
      "Epoch 84/100\n",
      "7794/7794 [==============================] - 0s 56us/step - loss: 1.6959 - acc: 0.4397 - val_loss: 1.9285 - val_acc: 0.4163\n",
      "Epoch 85/100\n",
      "7794/7794 [==============================] - 0s 62us/step - loss: 1.6889 - acc: 0.4468 - val_loss: 1.9275 - val_acc: 0.4230\n",
      "Epoch 86/100\n",
      "7794/7794 [==============================] - 0s 51us/step - loss: 1.6786 - acc: 0.4462 - val_loss: 1.9288 - val_acc: 0.4209\n",
      "Epoch 87/100\n",
      "7794/7794 [==============================] - 0s 53us/step - loss: 1.6963 - acc: 0.4434 - val_loss: 1.9276 - val_acc: 0.4194\n",
      "Epoch 88/100\n",
      "7794/7794 [==============================] - 0s 55us/step - loss: 1.6774 - acc: 0.4523 - val_loss: 1.9362 - val_acc: 0.4204\n",
      "Epoch 89/100\n",
      "7794/7794 [==============================] - 0s 57us/step - loss: 1.6868 - acc: 0.4420 - val_loss: 1.9323 - val_acc: 0.4179\n",
      "Epoch 90/100\n",
      "7794/7794 [==============================] - 0s 63us/step - loss: 1.6797 - acc: 0.4515 - val_loss: 1.9320 - val_acc: 0.4174\n",
      "Epoch 91/100\n",
      "7794/7794 [==============================] - 0s 62us/step - loss: 1.6825 - acc: 0.4477 - val_loss: 1.9303 - val_acc: 0.4189\n",
      "Epoch 92/100\n",
      "7794/7794 [==============================] - 0s 51us/step - loss: 1.6800 - acc: 0.4461 - val_loss: 1.9373 - val_acc: 0.4215\n",
      "Epoch 93/100\n",
      "7794/7794 [==============================] - 0s 55us/step - loss: 1.6891 - acc: 0.4478 - val_loss: 1.9321 - val_acc: 0.4235\n",
      "Epoch 94/100\n",
      "7794/7794 [==============================] - 1s 68us/step - loss: 1.6930 - acc: 0.4425 - val_loss: 1.9324 - val_acc: 0.4276\n",
      "Epoch 95/100\n",
      "7794/7794 [==============================] - 1s 66us/step - loss: 1.6886 - acc: 0.4497 - val_loss: 1.9358 - val_acc: 0.4281\n",
      "Epoch 96/100\n",
      "7794/7794 [==============================] - 0s 50us/step - loss: 1.6778 - acc: 0.4473 - val_loss: 1.9339 - val_acc: 0.4240\n",
      "Epoch 97/100\n",
      "7794/7794 [==============================] - 1s 66us/step - loss: 1.6747 - acc: 0.4410 - val_loss: 1.9324 - val_acc: 0.4251\n",
      "Epoch 98/100\n",
      "7794/7794 [==============================] - 0s 47us/step - loss: 1.6822 - acc: 0.4443 - val_loss: 1.9364 - val_acc: 0.4266\n",
      "Epoch 99/100\n",
      "7794/7794 [==============================] - 1s 65us/step - loss: 1.6752 - acc: 0.4488 - val_loss: 1.9349 - val_acc: 0.4240\n",
      "Epoch 100/100\n",
      "7794/7794 [==============================] - 0s 50us/step - loss: 1.6688 - acc: 0.4515 - val_loss: 1.9369 - val_acc: 0.4215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f14e3eab940>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model(maxlen, num_classes):\n",
    "    # Text input\n",
    "    sequence_input = Input(shape=(maxlen,), dtype='int32')\n",
    "    # Embedding layer\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    # Average layer\n",
    "    X = Lambda(lambda x: K.mean(x, axis=1))(embedded_sequences)\n",
    "    # Dense layer\n",
    "    X = Dense(128, activation='relu')(X)\n",
    "    # Dropout\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Softmax layer\n",
    "    output = Dense(num_classes, activation='softmax')(X)\n",
    "    \n",
    "    return Model(sequence_input, output)\n",
    "   \n",
    "# Create the network\n",
    "model = model(maxlen, num_classes)  \n",
    "\n",
    "# Choose an Adam optimizer\n",
    "opt = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=100, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve poor performance on the training set... less than 50%. Unfortunetaly, I took way too much time figuring out how to collect and format the data to feed it to the neural net so I don't have more time to spend on improving my performance.\n",
    "\n",
    "The performance on the training and validation sets are fairly close which suggests we do not suffer overfitting.\n",
    "\n",
    "The learning seems to start pretty well, but then stagnates really fast. We clearly have a bias problem which could potentially be solved by 3 different methods:\n",
    "\n",
    "* Gather more data since our training set might be not big enough to achieve such task.\n",
    "* Train a bigger network, but we need to be careful not to overfit the training set. If we train a network that is too big, we will also have to be careful about the problem of vanishing/exploding gradients.\n",
    "* Try a different NN architecture, maybe try RNNs after all? In that case, it would be better to use a bidirectionnal LSTM right away since these nets have proven to be the most performant, while avoiding to some point the problem of vanishing/exploding gradients.\n",
    "\n",
    "One thing that I wonder is if we could focus our analysis on more specific words. For now, we don't perform any filtering, and words like \"the\", \"and\", etc. are used to often that it may impact our performance. On the other hand, specific terminology like the name of a specific disease may give much more confidence that an article is actually about medicine.\n",
    "\n",
    "**How would you make your classifier available as a web service?**\n",
    "I have no experience on how to make such a system available as a web service. However, I know that Tensorflow is a stable framework for production uses. We could also make this model train on the newly posted submissions by changing the optimizer to stochastic gradient descent.\n",
    "\n",
    "**Would it be possible to improve the results using the content of a submission's comments? (bear in mind r/Science is highly moderated and non-topical discussions are removed) If so, how?**\n",
    "If the comments on a post are highly-moderated and are relevant to the post's topic, then, yes I think it would be possible to use the comments to improve our system. One thing we could do is choose an arbitrary number of comments to consider: maybe 5 comments for example. To avoid any input size problem, we could concatenate the comments with the post title. As long as the comments are relevant to the post, it should reinforce the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepflair",
   "language": "python",
   "name": "deepflair"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
